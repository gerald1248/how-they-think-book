<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Chapter 7: Tokens &#8212; How They Think</title>
  <meta property="og:title" content="Chapter 7: Tokens ‚Äî How They Think">
  <meta property="og:description" content="An online book about how ChatGPT works">
  <meta property="og:image" content="https://ericsilberstein1.github.io/how-they-think-book/images/ogimage.png">
  <meta property="og:type" content="article">
  <meta name="twitter:card" content="summary_large_image">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Raleway:wght@300;400;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <nav class="chapter-nav">
    <a href="chapter-6.html">&#8592; Previous</a>
    <a href="index.html">Contents</a>
    <a href="chapter-8.html">Next &#8594;</a>
  </nav>
  <article>
    <div class="chapter-heading">
      <p class="chapter-number">7</p>
      <hr class="chapter-rule">
      <h1>Tokens</h1>
    </div>
    <p>Subway tokens went away a long time ago, but tokens in games, cryptographic tokens, tokenization of virtual goods, and tokenization of personally identifiable information are all ways you may have encountered tokens recently. Another way is paying for use of models (e.g. from OpenAI and Anthropic) where you pay a tiny fraction of a cent per token. The funny thing is that these tokens are not a virtual currency made up by the vendors as a way to meter their services. Tokens are in fact a fundamental concept: GPT models are trained on tokens, and once trained, they take tokens as input and generate tokens as output.</p>
    <p>So what is a token? For the moment, think of it as a word. Take the example in <a href="chapter-2.html" class="xref">chapter 2</a> of a language with very few words and assign each word an id:</p>
    <figure id="tbl-7-1" style="max-width:50%">
      <input type="checkbox" id="lb-57" class="lightbox-toggle">
      <label for="lb-57"><img src="images/RC052.png" alt="" loading="lazy"></label>
      <label for="lb-57" class="lightbox-overlay"><img src="images/RC052.png" alt=""></label>
      <figcaption>Table 7.1. Example of assigning IDs to words.</figcaption>
    </figure>
    <p>In <a href="chapter-5.html" class="xref">chapter 5</a> you hopefully built an appreciation for how when we were training to learn the weights for our turkey feather model, and later when we were plugging turkeys into the model to get out a prediction, we worked with matrices of numbers. The same is true when working with language. If we‚Äôre going to want to feed the sentence ‚ÄúHe went to the store‚Äù into a model as training data, we‚Äôll want to first convert it to numbers. A <strong>tokenizer</strong> encodes text as a list of tokens. In this case: 2, 10, 9, 8, 7. (These days, images, movies, and sound can also be encoded as tokens.)</p>
    <p>A tokenizer can also decode tokens into text. For example, the model might generate the tokens: 8, 1, 4, 6 which will get converted to ‚ÄúThe bed is red‚Äù and shown to the user.</p>
    <p>You‚Äôll notice that I didn‚Äôt account for capitalization, the spaces between words, or the period at the end of the sentence. We will need to since capitalization, spaces and punctuation are an important part of language, and you‚Äôll understand how by the end of this chapter.</p>
    <p>Soon we‚Äôre going to talk about a <strong>byte-pair encoding</strong> tokenizer. To get ready, I‚Äôll explain what a byte is and how text is represented in digital form, completely aside from tokenizers and AI.</p>
    <p>Digital data is 1s and 0s. Think about the information stored in your computer, the information being processed by its CPU or GPU, or the information being sent over a network‚Äîit‚Äôs all 1s and 0s. Each 1 or 0 is a <strong>bit</strong>. A (modern) <strong>byte</strong> is always 8 bits. A kilobyte is 1024 bytes. A megabyte is 1024 kilobytes. When someone sends you a 1 megabyte photo, they are sending you 1024√ó1024√ó8 = 8,388,608 bits. That‚Äôs 2<sup>23</sup> bits. Computers like powers of two.</p>
    <p>Since a single byte is 8 bits ‚Äúnext to each other,‚Äù it could be eight zeros (00000000) or eight ones (11111111) or any other combination of zeros and ones (01000001). With two choices for the first position, two choices for the next position, and so on through the eighth position, we‚Äôll end up with 2<sup>8</sup> = 256 possibilities. Since it‚Äôs tedious to write eight 1s and 0s every time we want to write a byte, we can refer to bytes as 0, 1, 2, through 255 referring to 00000000, 00000001, 00000010, all the way to 11111111.</p>
    <p>(You might be thinking, if eight bits can only represent 256 different things, how are the ‚Äúmere‚Äù 8,338,608 bits in our one megabyte photo enough to represent detailed images of any scene, real or imagined. You have to think about the power of powers. Two raised to the power of 8,388,608 is a number with over 2.5 million digits.)</p>
    <p>The core of a computer works with bits and bytes. Bytes are stored, bytes are processed, and the instructions of how to process the bytes are themselves in bytes. Nothing at the deepest level of your computer knows anything about letters in English or any other language. This is as true for the digital computers of today as it was of the first digital computers. Therefore, to represent text, there needs to be a system to encode letters as bytes. It‚Äôs a much more straightforward problem than tokenizing (which we‚Äôll come back to), and yet filled with subtle issues you might not think of.</p>
    <p>The history of using two states to encode letters predates computers. Morse code, for example, was created around 1840 and used variable numbers of dots and dashes to encode letters, numbers, and some punctuation. When one of the co-creators of Morse code was deciding which letters should get shorter sequences of dots and dashes, he estimated the frequency of letters by counting the number of pieces of movable type for each letter at a newspaper printer. We‚Äôre going to see pretty much this exact technique again soon for how we decide our tokens.</p>
    <figure id="fig-7-1" style="max-width:90%">
      <input type="checkbox" id="lb-58" class="lightbox-toggle">
      <label for="lb-58"><img src="images/RC382.png" alt="" loading="lazy"></label>
      <label for="lb-58" class="lightbox-overlay"><img src="images/RC382.png" alt=""></label>
      <figcaption>Figure 7.1. The American Morse Code from page 470 of the 1867 book <em>The Telegraph Manual: A Complete History and Description of the Semaphoric, Electric and Magnetic Telegraphs of Europe, Asia, Africa, and America, Ancient and Modern. </em></figcaption>
    </figure>
    <p>Back to letters and bytes, you may have heard of ASCII which is a widely adopted standard from the 1960s. In it, uppercase A is encoded as 65, uppercase B as 66, lower case ‚Äòa‚Äô as 97, and exclamation point as 33. ASCII was designed to fit within seven bits which gave 128 possible values. A hundred twenty eight values was more than enough to represent the regular English alphabet and woefully inadequate to encode all characters in all languages, which these days even includes emoji.</p>
    <p>There is a long, wonderful, and fascinating history of people inventing encodings to handle different languages and standards bodies creating national and international standards. In the 1980s computer scientists and linguists from around the world got together to create a single worldwide standard (Unicode) to represent all characters. When I started my translation management software company, encodings were still all over the map and getting software to support languages like Japanese meant special logic to handle what were called double byte character sets. I‚Äôll spare you all that history. Today, UTF-8, a clever encoding of Unicode that is backwards compatible with ASCII, is widely used. This limits the scope of our trouble. Our tokenizer will need to convert from UTF-8 text to tokens and from tokens back to UTF-8. We can skip worrying about other ways of representing characters.</p>
    <p>Unlike ASCII, UTF-8 uses different numbers of bytes to encode different characters. I‚Äôll show you a few examples so you can get the idea:</p>
    <figure id="tbl-7-2">
      <input type="checkbox" id="lb-59" class="lightbox-toggle">
      <label for="lb-59"><img src="images/RC053.png" alt="" loading="lazy"></label>
      <label for="lb-59" class="lightbox-overlay"><img src="images/RC053.png" alt=""></label>
      <figcaption>Table 7.2. Examples of Unicode characters encoded in UTF-8.</figcaption>
    </figure>
    <p>There are nearly 300,000 Unicode characters. Ideally our GPT model will never encounter a character that causes it to stumble, but most of them are also useless because they are so rare.</p>
    <p>Now that you know about bits, bytes, and UTF-8, let‚Äôs remember what we‚Äôre trying to achieve here. We‚Äôre going to be building a model, a prediction machine, that takes in text and generates text. Since the machine is going to operate on matrices of numbers, we need to first turn our text into numbers, and later we‚Äôll need to turn those numbers back into text.</p>
    <p>Well, didn‚Äôt we just solve that problem? We can use UTF-8. So back to our example sentence from above: ‚ÄúHe went to the store.‚Äù We would start with the UTF-8 byte for ‚ÄúH,‚Äù then for ‚Äúe,‚Äù then for space, etc giving us: 72, 101, 32, 119, 101, 110, 116, 32, 116, 111, 32, 116, 104, 101, 32, 115, 116, 111, 114, 101, 46. If you count, that‚Äôs a total of 21 bytes corresponding to the 16 letters, four spaces, and one period in the sentence.</p>
    <p>You could try this and it would work to an extent. The concept and the beautiful thing about training hundreds of millions, or billions, or trillions of weights is that the model has a lot of room to find a way to make the predictions you‚Äôre asking for. So if we feed in lots of data (think back to X and Y in the turkey example) where the sequence 72, 101 (He) predicts 32 (space), it will certainly learn that.</p>
    <p>And yet, and this is a little frustrating, for a given number of weights, the model will work much, much better if the input and output tokens are bigger units that have more meaning than letters, like words. We really want ‚ÄúHe,‚Äù ‚Äúwent,‚Äù and ‚Äústore‚Äù to be individual tokens. On the other hand, it doesn‚Äôt do us much good for ‚Äúfloccinaucinihilipilification‚Äù to be a token, and in fact it harms us because we‚Äôre going to be doing many computations that scale with the number of unique tokens. Think of tokens as an expensive resource not to be wasted on words that will rarely appear either in training or as future input to the trained model.</p>
    <p>In our turkey model, the first and only step was to multiply the turkey data (height and length) by our weights. We‚Äôll come to how our GPT model works later, but know that the first of many steps is to expand each token into a long list of numbers (known as an embedding) that in some sense capture the meaning of the token. This will work much better if the tokens have meaning, definitions, and richness beyond individual letters. Think ‚ÄúHe,‚Äù ‚Äústore,‚Äù &quot;motorcycle,&quot;  ‚ÄúÂ±±‚Äù (mountain), rather than ‚ÄúH‚Äù or ‚Äús.‚Äù</p>
    <p>I realize that what I‚Äôm saying could sound like circular reasoning or even the tail wagging the dog. It‚Äôs a choice of the model designer to start by expanding each token into an embedding, so why not just leave this out and keep the tokens simple? The answer is that useful models are designed through insights, theories, experimentation, and iteration. Empirically, word-like tokens and embeddings work well.</p>
    <p>A few years ago there was a lot of talk and amusement about how early versions of GPT models would get this question wrong: ‚ÄúHow many r‚Äôs are in strawberry?‚Äù Most people will get that right, and a tiny, simple computer program from half a century ago would always get it right, so it was strange that with insane amounts of computation by historical standards, a powerful model couldn‚Äôt answer this simple question. Now you can start to see why. ‚ÄúStrawberry‚Äù might be represented as a token, or a few tokens (we‚Äôll come to that), but not as its individual letters. So to answer the simple question, the model needs to have learned how to spell in letters what to it is a token or two.</p>
    <p>Somehow we want to end up with an appropriate number of tokens. Let‚Äôs call this number V for vocabulary. We want V to be big enough so that lots of common words will each become their own token. But, for the reason I mentioned above, V can‚Äôt be too big. V also needs to be proportional to the overall size of the model. For example, a small model would be unable to learn how to predict which of a million unique tokens should come next, even though a million is probably the right number to cover the most common words in the most common languages put together.</p>
    <p>How should we pick V? If we had all the time in the world we would run experiments. We could train a model with different sizes of V and see what works best. In chapters 18 and 25 you‚Äôll learn how to evaluate the model and this will give you a firm idea of how to run these experiments. However, we don‚Äôt need to start from scratch. We can look at what works well in other models and look to ratios published by researchers. For the 20-layer model I trained ahead of time, and for the 32-layer model we‚Äôll be training together starting in <a href="chapter-24.html" class="xref">chapter 24</a>, we‚Äôre going to set V to 65,536 which is 2<sup>16</sup>.</p>
    <p>Another thing that would be really nice is if no matter what text we give our tokenizer, it can always encode it in a lossless fashion. This means that if we give it a sentence consisting of an obscure word in English, an obscure Chinese character, and an obscure emoji, and then we take the token IDs that come out and ask it to decode them, we‚Äôll get back to what we started with. In other words we want this:</p>
    <figure id="fig-7-2">
      <input type="checkbox" id="lb-60" class="lightbox-toggle">
      <label for="lb-60"><img src="images/RC054.png" alt="" loading="lazy"></label>
      <label for="lb-60" class="lightbox-overlay"><img src="images/RC054.png" alt=""></label>
      <figcaption>Figure 7.2. A lossless tokenizer.</figcaption>
    </figure>
    <p>Not:</p>
    <figure id="fig-7-3">
      <input type="checkbox" id="lb-61" class="lightbox-toggle">
      <label for="lb-61"><img src="images/RC055.png" alt="" loading="lazy"></label>
      <label for="lb-61" class="lightbox-overlay"><img src="images/RC055.png" alt=""></label>
      <figcaption>Figure 7.3. A lossy tokenizer.</figcaption>
    </figure>
    <p>It used to be common practice for tokenizers to encode words not in the vocabulary as <em>unknown</em> like in <a href="#fig-7-3" class="xref" data-preview="images/RC055.png">figure 7.3</a>. The problem is when you want to train a model on all text you can possibly find and that‚Äôs billions or even trillions of characters out in the wild, you‚Äôre going to see even rare things, and we don‚Äôt want to deprive the model of the chance to learn from or generate those rare things. If our tokenizer is lossy and outputs <em>unknown</em> tokens, then ‚Äúfloccinaucinihilipilification,‚Äù  ‚ÄúÈåî,‚Äù and ‚Äúü¶É‚Äù will all look identical to the model: the unknown token.</p>
    <p>Since our vocab size is not going to be infinite, we want to choose our tokens in an optimal way. For example, should ‚Äúrunning‚Äù get its own token? Maybe yes if our vocab size is 50,000 but not if it‚Äôs 5,000. If it doesn‚Äôt get its own token, do we back off to treating it as seven letters, or could we represent it as ‚Äúrun‚Äù and ‚Äún‚Äù and ‚Äúing‚Äù so that the model can make use of the meaning of ‚Äúrun‚Äù and the meaning of ‚Äúing‚Äù? There‚Äôs no one right way to do this.</p>
    <p>I‚Äôm going to describe an approach that works well and is elegant: a <strong>byte-level byte pair tokenizer</strong>. Yes, that‚Äôs a mouthful. You can call it a <strong>BPE</strong>. By elegant, I mean that the logic behind it is simple. It‚Äôs not filled with hand-crafted rules or per-language techniques to break words up into morphemes and prefixes. In other words, if ‚Äúrunning‚Äù gets split up as I described in the previous paragraph, it will happen naturally, not because a linguist hardcoded that ‚Äúing‚Äù is a useful suffix. I‚Äôll start by walking through a tiny and slightly simplified example.</p>
    <p>With a BPE tokenizer, once our tokenizer is all trained, and we‚Äôre going along encoding text, say the word ‚Äúrunning‚Äù to stick with the example from above, if we don‚Äôt have a token for the whole word, or even ‚Äúrun,‚Äù we‚Äôll back all the way up to encoding it as seven tokens, one for each letter. We therefore need a minimum of 256 tokens, one for each byte. Since my goal right now is to show a tiny example, I‚Äôm going to choose a vocab size of 261, five more than the minimum. And just to be totally clear, in real life we would never choose 261 for V.</p>
    <p>Let‚Äôs suppose we‚Äôve collected all the text we could from out in the world and found these sentences:</p>
    <ul>
      <li>she runs.</li>
      <li>he also runs.</li>
      <li>she is running.</li>
    </ul>
    <p>We start by encoding each word using the tokens we have so far, which at this point is exactly the same as encoding the words in UTF-8.</p>
    <figure id="tbl-7-3" style="max-width:90%">
      <input type="checkbox" id="lb-62" class="lightbox-toggle">
      <label for="lb-62"><img src="images/RC056.png" alt="" loading="lazy"></label>
      <label for="lb-62" class="lightbox-overlay"><img src="images/RC056.png" alt=""></label>
      <figcaption>Table 7.3. First step is to encode each word using our starting set of tokens.</figcaption>
    </figure>
    <p>Now we look at all the consecutive pairs of tokens: 115 and 104 (corresponding to ‚Äúsh‚Äù), 104 and 101 (corresponding to ‚Äúhe‚Äù), 114 and 117 (corresponding to ‚Äúru‚Äù) and so on, and find the pair with the highest frequency. You can trust me that there are a few tied for first place. We‚Äôll pick the first of these, 104 and 101 (corresponding to ‚Äúhe‚Äù) and <strong>merge </strong>them into a new token which we‚Äôll give the next ID, 256. (Why is the next ID 256 and not 257? Because our first 256 tokens went from 0 to 255.)</p>
    <figure id="tbl-7-4">
      <input type="checkbox" id="lb-63" class="lightbox-toggle">
      <label for="lb-63"><img src="images/RC298.png" alt="" loading="lazy"></label>
      <label for="lb-63" class="lightbox-overlay"><img src="images/RC298.png" alt=""></label>
      <figcaption>Table 7.4. Token pair 104 and 101 become the single token 256.</figcaption>
    </figure>
    <p>Notice that we replace the pair of tokens 104 and 101 with the single token 256 everywhere. Next up we merge pair 114 and 117 (‚Äúru‚Äù in both ‚Äúruns‚Äù and ‚Äúrunning‚Äù) into new token 257. We keep going in this way until we get to the token with ID 260. (This gives us a total vocab size of 261 since we start at 0.)</p>
    <p>At the end of this very short tokenizing training our list of words looks like this:</p>
    <figure id="tbl-7-5" style="max-width:90%">
      <input type="checkbox" id="lb-64" class="lightbox-toggle">
      <label for="lb-64"><img src="images/RC058.png" alt="" loading="lazy"></label>
      <label for="lb-64" class="lightbox-overlay"><img src="images/RC058.png" alt=""></label>
      <figcaption>Table 7.5: Our words and their encodings in tokens after we stop merging.</figcaption>
    </figure>
    <p>And here‚Äôs our list of tokens:</p>
    <figure id="tbl-7-6" style="max-width:60%">
      <input type="checkbox" id="lb-65" class="lightbox-toggle">
      <label for="lb-65"><img src="images/RC059.png" alt="" loading="lazy"></label>
      <label for="lb-65" class="lightbox-overlay"><img src="images/RC059.png" alt=""></label>
      <figcaption>Table 7.6. Final list of tokens.</figcaption>
    </figure>
    <p>We can now encode ‚Äúhe runs‚Äù as 256, 260 (glossing over spaces for the moment) and we can also encode the fire emoji even though it wasn‚Äôt in our tokenizer training sentences. It will be 240, 159, 148, 165. (See <a href="#tbl-7-2" class="xref" data-preview="images/RC053.png">table 7.2</a> for where I got those token IDs.)</p>
    <hr class="section-break">
    <p>Now it‚Äôs time to do this on a bigger scale so we can create a tokenizer for a vocab size of 65,536. I‚Äôll be using the resulting tokenizer for examples in future chapters and we‚Äôll use it when we train our 32-layer model starting in <a href="chapter-24.html" class="xref">chapter 24</a>. To train the tokenizer, we‚Äôll need enough sample text from out in the world to be confident we‚Äôre making valid decisions about what token pairs to merge. In the tiny example above, we completely merged ‚Äúrun‚Äù into a single token but ran out of vocab size to merge ‚Äúi‚Äù and ‚Äús‚Äù into ‚Äúis.‚Äù If we had a larger sample of English text, I‚Äôm confident ‚Äúis‚Äù would be more common than ‚Äúru‚Äù and would have been merged first.</p>
    <p>We could scrape web pages to collect text, except we don‚Äôt need to, because it‚Äôs already been done. Common Crawl is a non-profit organization that maintains a free, open repository of web crawl data. You‚Äôll see examples later. For now, know that I used around four billion characters worth of text to train the tokenizer. Four billion may sound like a big number, but it‚Äôs small compared to the amount of text we‚Äôll be using later to train our model. Also, with a reasonable modern internet connection, it will only take a few minutes to find and download that much text.</p>
    <p>Another thing I glossed over before but we have to deal with in the real world is breaking text up into words. I listed three sentences above (‚Äúshe runs,‚Äù etc.) and then listed their words in <a href="#tbl-7-3" class="xref" data-preview="images/RC056.png">table 7.3</a> without ever being clear about how I came up with the words. I also didn‚Äôt say what happened to the spaces or periods, and of course in the real world text we‚Äôll see numbers, hyphenated words, quotes, apostrophes, parenthesis, and much more. We somehow have to be able to take any text and break it into words. These may not be words as we think of them as English, but they serve as boundaries beyond which we don‚Äôt merge.</p>
    <p>This breaking up step is one place we use hardcoded rules. Rather than try to describe them all, I‚Äôll show a few rules by example.</p>
    <figure id="tbl-7-7">
      <input type="checkbox" id="lb-66" class="lightbox-toggle">
      <label for="lb-66"><img src="images/RC060.png" alt="" loading="lazy"></label>
      <label for="lb-66" class="lightbox-overlay"><img src="images/RC060.png" alt=""></label>
      <figcaption>Table 7.7. Example of how text is split into words.</figcaption>
    </figure>
    <p>You may be surprised that spaces are often included in words. This means through merging we‚Äôll end up with many tokens that start with spaces. Apparently this works well. Ending punctuation like period and exclamation points become their own words. Apostrophe-s also becomes its own word, which is possibly intuitive because the model can learn that the apostrophe-s token means possession. </p>
    <p>And finally, here are a few tokens from my trained tokenizer. The more common words have lower IDs.</p>
    <figure id="tbl-7-8" style="max-width:50%">
      <input type="checkbox" id="lb-67" class="lightbox-toggle">
      <label for="lb-67"><img src="images/RC061.png" alt="" loading="lazy"></label>
      <label for="lb-67" class="lightbox-overlay"><img src="images/RC061.png" alt=""></label>
      <figcaption>Table 7.8. A few of our 65,000+ tokens. These are the tokens we‚Äôll use to train our 32-layer model later in this book.</figcaption>
    </figure>
    <p>Encoding examples:</p>
    <figure id="tbl-7-9" style="max-width:60%">
      <input type="checkbox" id="lb-68" class="lightbox-toggle">
      <label for="lb-68"><img src="images/RC062.png" alt="" loading="lazy"></label>
      <label for="lb-68" class="lightbox-overlay"><img src="images/RC062.png" alt=""></label>
      <figcaption>Table 7.9. Examples of tokenizing text.</figcaption>
    </figure>
    <p>We‚Äôre just about done with the tokenizer and ready to move on to using the tokens. One last point to keep in mind: Since our model will take tokens in and predict tokens, it‚Äôs going to turn out to be helpful to have special, reserved tokens that signify specific information that can never appear in normal text. One example is <em>beginning of sequence.</em> There will be others. The point for now is to leave room for them so that the total vocab size still ends up at our desired number, 65,536 in this case.</p>
    <figure id="tbl-7-10" style="max-width:60%">
      <input type="checkbox" id="lb-69" class="lightbox-toggle">
      <label for="lb-69"><img src="images/RC063.png" alt="" loading="lazy"></label>
      <label for="lb-69" class="lightbox-overlay"><img src="images/RC063.png" alt=""></label>
      <figcaption>Table 7.10. The last regular token and our first of nine special tokens.</figcaption>
    </figure>
    <p>That‚Äôs it! Tokenizer in hand, on to the GPT.</p>
  </article>
  <nav class="chapter-nav">
    <a href="chapter-6.html">&#8592; Previous</a>
    <a href="index.html">Contents</a>
    <a href="chapter-8.html">Next &#8594;</a>
  </nav>
  <footer class="site-footer"><em>How They Think</em> &mdash; February 12, 2026 draft &mdash; &copy; Eric Silberstein</footer>
  <div id="preview-tooltip" class="preview-tooltip" hidden>
    <img id="preview-tooltip-img" src="" alt="">
  </div>
  <script>
  (function() {
    var tip = document.getElementById('preview-tooltip');
    var img = document.getElementById('preview-tooltip-img');
    var links = document.querySelectorAll('a.xref[data-preview]');
    var hideTimer;
    function show(e) {
      clearTimeout(hideTimer);
      var src = e.currentTarget.getAttribute('data-preview');
      if (img.getAttribute('src') !== src) img.setAttribute('src', src);
      tip.removeAttribute('hidden');
      position(e);
    }
    function position(e) {
      var rect = e.currentTarget.getBoundingClientRect();
      var tw = Math.min(480, window.innerWidth - 32);
      var left = rect.left + rect.width / 2 - tw / 2;
      if (left < 8) left = 8;
      if (left + tw > window.innerWidth - 8) left = window.innerWidth - 8 - tw;
      tip.style.width = tw + 'px';
      tip.style.left = left + 'px';
      var above = rect.top - 8;
      var below = window.innerHeight - rect.bottom - 8;
      if (above > below) {
        tip.style.bottom = (window.innerHeight - rect.top + 8) + 'px';
        tip.style.top = 'auto';
      } else {
        tip.style.top = (rect.bottom + 8) + 'px';
        tip.style.bottom = 'auto';
      }
    }
    function hide() {
      hideTimer = setTimeout(function() { tip.setAttribute('hidden', ''); }, 120);
    }
    links.forEach(function(a) {
      a.addEventListener('mouseenter', show);
      a.addEventListener('mouseleave', hide);
    });
    tip.addEventListener('mouseenter', function() { clearTimeout(hideTimer); });
    tip.addEventListener('mouseleave', hide);
    document.addEventListener('keydown', function(e) {
      if (e.key === 'Escape') {
        var open = document.querySelector('.lightbox-toggle:checked');
        if (open) open.checked = false;
      }
    });
  })();
  </script>
</body>
</html>
