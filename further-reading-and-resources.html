<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Further reading and resources &#8212; How They Think</title>
  <meta property="og:title" content="Further reading and resources — How They Think">
  <meta property="og:description" content="An online book about how ChatGPT works">
  <meta property="og:image" content="https://ericsilberstein1.github.io/how-they-think-book/images/ogimage.png">
  <meta property="og:type" content="article">
  <meta name="twitter:card" content="summary_large_image">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Raleway:wght@300;400;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <nav class="chapter-nav">
    <a href="select-figures.html">&#8592; Previous</a>
    <a href="index.html">Contents</a>
    <a href="feedback.html">Next &#8594;</a>
  </nav>
  <article>
    <div class="chapter-heading">
      <h1>Further reading and resources</h1>
    </div>
    <p><strong>On the transformer and GPT architecture</strong> (chapters 6, 11, 13–17, and 22):</p>
    <p>Original transformer: Vaswani, Ashish, Noam Shazeer, Niki Parmar, et al. “Attention Is All You Need.” arXiv:1706.03762. Preprint, arXiv, August 2, 2023. <a href="https://doi.org/10.48550/arXiv.1706.03762" target="_blank">https://doi.org/10.48550/arXiv.1706.03762</a>.</p>
    <p>Nanochat: Karpathy, Andrej. nanochat: The best ChatGPT that $100 can buy. 2025. <a href="https://github.com/karpathy/nanochat" target="_blank">https://github.com/karpathy/nanochat</a>.</p>
    <p>RMSNorm: Zhang, Biao, and Rico Sennrich. “Root Mean Square Layer Normalization.” arXiv:1910.07467. Preprint, arXiv, October 16, 2019. <a href="https://doi.org/10.48550/arXiv.1910.07467" target="_blank">https://doi.org/10.48550/arXiv.1910.07467</a>.</p>
    <p>Rotary Embeddings: Su, Jianlin, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. “RoFormer: Enhanced Transformer with Rotary Position Embedding.” arXiv:2104.09864. Preprint, arXiv, November 8, 2023. <a href="https://doi.org/10.48550/arXiv.2104.09864" target="_blank">https://doi.org/10.48550/arXiv.2104.09864</a>.</p>
    <p>LayerNorm: Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. “Layer Normalization.” arXiv:1607.06450. Preprint, arXiv, July 21, 2016. <a href="https://doi.org/10.48550/arXiv.1607.06450" target="_blank">https://doi.org/10.48550/arXiv.1607.06450</a>.</p>
    <p>BatchNorm: Ioffe, Sergey, and Christian Szegedy. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” arXiv:1502.03167. Preprint, arXiv, March 2, 2015. <a href="https://doi.org/10.48550/arXiv.1502.03167" target="_blank">https://doi.org/10.48550/arXiv.1502.03167</a>.</p>
    <p>Silberstein, Eric. “Tracing the Transformer in Diagrams.” <em>Towards Data Science, </em>November 7, 2024. <a href="https://towardsdatascience.com/tracing-the-transformer-in-diagrams-95dbeb68160c/" target="_blank">https://towardsdatascience.com/tracing-the-transformer-in-diagrams-95dbeb68160c/</a>.</p>
    <p>Apple II floating point routines: Woz 6502 Floating Point Routines from <em>Apple II Reference Manual (Red Book)</em>. January 1978, pages 94-95. <a href="http://6502.org/source/floats/wozfp3.txt" target="_blank">http://6502.org/source/floats/wozfp3.txt</a>.</p>
    <p>PyTorch documentation: <a href="https://docs.pytorch.org/docs/stable/" target="_blank">https://docs.pytorch.org/docs/stable/</a>.</p>
    <p><strong>On Adam and Muon optimizers </strong>(chapters 20 and 21):</p>
    <p>Adam: Kingma, Diederik P., and Jimmy Ba. “Adam: A Method for Stochastic Optimization.” arXiv:1412.6980. Preprint, arXiv, January 30, 2017. <a href="https://doi.org/10.48550/arXiv.1412.6980" target="_blank">https://doi.org/10.48550/arXiv.1412.6980</a>.</p>
    <p>AdamW: Loshchilov, Ilya, and Frank Hutter. “Decoupled Weight Decay Regularization.” arXiv:1711.05101. Preprint, arXiv, January 4, 2019. <a href="https://doi.org/10.48550/arXiv.1711.05101" target="_blank">https://doi.org/10.48550/arXiv.1711.05101</a>.</p>
    <p>Muon: Jordan, Keller et al. Muon: An optimizer for hidden layers in neural networks. 2024. <a href="https://kellerjordan.github.io/posts/muon/" target="_blank">https://kellerjordan.github.io/posts/muon/</a>.</p>
    <p><strong>On image recognition and image recognition models</strong> (chapters 12 and 22):</p>
    <p>ImageNet: Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. <em>ImageNet: A Large-Scale Hierarchical Image Database</em>. n.d.</p>
    <p>ImageNet: Russakovsky, Olga, Jia Deng, Hao Su, et al. “ImageNet Large Scale Visual Recognition Challenge.” arXiv:1409.0575. Preprint, arXiv, January 30, 2015. <a href="https://doi.org/10.48550/arXiv.1409.0575" target="_blank">https://doi.org/10.48550/arXiv.1409.0575</a>.</p>
    <p>AlexNet: Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “ImageNet Classification with Deep Convolutional Neural Networks.” <em>Communications of the ACM</em> 60, no. 6 (2017): 84–90. <a href="https://doi.org/10.1145/3065386" target="_blank">https://doi.org/10.1145/3065386</a>.</p>
    <p>ResNet: He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. “Deep Residual Learning for Image Recognition.” arXiv:1512.03385. Preprint, arXiv, December 10, 2015. <a href="https://doi.org/10.48550/arXiv.1512.03385" target="_blank">https://doi.org/10.48550/arXiv.1512.03385</a>.</p>
    <p>Inception: Szegedy, Christian, Wei Liu, Yangqing Jia, et al. “Going Deeper with Convolutions.” arXiv:1409.4842. Preprint, arXiv, September 17, 2014. <a href="https://doi.org/10.48550/arXiv.1409.4842" target="_blank">https://doi.org/10.48550/arXiv.1409.4842</a>.</p>
    <p><strong>On training and training and evaluation datasets</strong> (chapters 18 and 24–28):</p>
    <p>Chinchilla ratio: Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, et al. “Training Compute-Optimal Large Language Models.” arXiv:2203.15556. Preprint, arXiv, March 29, 2022. <a href="https://doi.org/10.48550/arXiv.2203.15556" target="_blank">https://doi.org/10.48550/arXiv.2203.15556</a>.</p>
    <p>CORE: Li, Jeffrey, Alex Fang, Georgios Smyrnis, et al. “DataComp-LM: In Search of the next Generation of Training Sets for Language Models.” arXiv:2406.11794. Preprint, arXiv, April 21, 2025. <a href="https://doi.org/10.48550/arXiv.2406.11794" target="_blank">https://doi.org/10.48550/arXiv.2406.11794</a>.</p>
    <p>SQuAD: Rajpurkar, Pranav, Jian Zhang, Konstantin Lopyrev, and Percy Liang. “SQuAD: 100,000+ Questions for Machine Comprehension of Text.” arXiv:1606.05250. Preprint, arXiv, October 11, 2016. <a href="https://doi.org/10.48550/arXiv.1606.05250" target="_blank">https://doi.org/10.48550/arXiv.1606.05250</a>.</p>
    <p>ARC: Clark, Peter, Isaac Cowhey, Oren Etzioni, et al. “Think You Have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge.” arXiv:1803.05457. Preprint, arXiv, March 14, 2018. <a href="https://doi.org/10.48550/arXiv.1803.05457" target="_blank">https://doi.org/10.48550/arXiv.1803.05457</a>.</p>
    <p>AGIEval: Zhong, Wanjun, Ruixiang Cui, Yiduo Guo, et al. “AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models.” arXiv:2304.06364. Preprint, arXiv, September 18, 2023. <a href="https://doi.org/10.48550/arXiv.2304.06364" target="_blank">https://doi.org/10.48550/arXiv.2304.06364</a>.</p>
    <p>HumanEval: Chen, Mark, Jerry Tworek, Heewoo Jun, et al. “Evaluating Large Language Models Trained on Code.” arXiv:2107.03374. Preprint, arXiv, July 14, 2021. <a href="https://doi.org/10.48550/arXiv.2107.03374" target="_blank">https://doi.org/10.48550/arXiv.2107.03374</a>.</p>
    <p>SmolTalk: Allal, Loubna Ben, Anton Lozhkov, Elie Bakouch, et al. “SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model.” arXiv:2502.02737. Preprint, arXiv, February 4, 2025. <a href="https://doi.org/10.48550/arXiv.2502.02737" target="_blank">https://doi.org/10.48550/arXiv.2502.02737</a>.</p>
    <p>Magpie: Xu, Zhangchen, Fengqing Jiang, Luyao Niu, et al. “Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing.” arXiv:2406.08464. Preprint, arXiv, October 7, 2024. <a href="https://doi.org/10.48550/arXiv.2406.08464" target="_blank">https://doi.org/10.48550/arXiv.2406.08464</a>.</p>
    <p>MMLU: Hendrycks, Dan, Collin Burns, Steven Basart, et al. “Measuring Massive Multitask Language Understanding.” arXiv:2009.03300. Preprint, arXiv, January 12, 2021. <a href="https://doi.org/10.48550/arXiv.2009.03300" target="_blank">https://doi.org/10.48550/arXiv.2009.03300</a>.</p>
    <p>My first time coding with a GPT model: Silberstein, Eric. “Playing with GPT-4: Writing Code.” <em>Klaviyo Engineering Blog.</em> March 25, 2023. <a href="https://klaviyo.tech/playing-with-gpt-4-writing-code-a137a7261655" target="_blank">https://klaviyo.tech/playing-with-gpt-4-writing-code-a137a7261655</a>.</p>
  </article>
  <nav class="chapter-nav">
    <a href="select-figures.html">&#8592; Previous</a>
    <a href="index.html">Contents</a>
    <a href="feedback.html">Next &#8594;</a>
  </nav>
  <footer class="site-footer"><em>How They Think</em> &mdash; February 12, 2026 draft &mdash; &copy; Eric Silberstein</footer>
  <div id="preview-tooltip" class="preview-tooltip" hidden>
    <img id="preview-tooltip-img" src="" alt="">
  </div>
  <script>
  (function() {
    var tip = document.getElementById('preview-tooltip');
    var img = document.getElementById('preview-tooltip-img');
    var links = document.querySelectorAll('a.xref[data-preview]');
    var hideTimer;
    function show(e) {
      clearTimeout(hideTimer);
      var src = e.currentTarget.getAttribute('data-preview');
      if (img.getAttribute('src') !== src) img.setAttribute('src', src);
      tip.removeAttribute('hidden');
      position(e);
    }
    function position(e) {
      var rect = e.currentTarget.getBoundingClientRect();
      var tw = Math.min(480, window.innerWidth - 32);
      var left = rect.left + rect.width / 2 - tw / 2;
      if (left < 8) left = 8;
      if (left + tw > window.innerWidth - 8) left = window.innerWidth - 8 - tw;
      tip.style.width = tw + 'px';
      tip.style.left = left + 'px';
      var above = rect.top - 8;
      var below = window.innerHeight - rect.bottom - 8;
      if (above > below) {
        tip.style.bottom = (window.innerHeight - rect.top + 8) + 'px';
        tip.style.top = 'auto';
      } else {
        tip.style.top = (rect.bottom + 8) + 'px';
        tip.style.bottom = 'auto';
      }
    }
    function hide() {
      hideTimer = setTimeout(function() { tip.setAttribute('hidden', ''); }, 120);
    }
    links.forEach(function(a) {
      a.addEventListener('mouseenter', show);
      a.addEventListener('mouseleave', hide);
    });
    tip.addEventListener('mouseenter', function() { clearTimeout(hideTimer); });
    tip.addEventListener('mouseleave', hide);
    document.addEventListener('keydown', function(e) {
      if (e.key === 'Escape') {
        var open = document.querySelector('.lightbox-toggle:checked');
        if (open) open.checked = false;
      }
    });
  })();
  </script>
</body>
</html>
